{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, shapiro\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy.random import uniform\n",
    "import missingno as msno\n",
    "# 自定义分隔符函数\n",
    "def custom_delimiter(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        # 使用正则表达式分隔数据\n",
    "        split_line = re.split(r'  +| (?=-)', line.strip())\n",
    "        data.append(split_line)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# 读取数据集\n",
    "data = custom_delimiter('data.txt')\n",
    "\n",
    "# 使用 df.info 函数分析数据\n",
    "print(\"/////////data info 基本信息如下////////\\n\")\n",
    "print(data.info())\n",
    "\n",
    "# 检查数据当中是否含有缺失值\n",
    "print(\"/////////缺失值统计如下////////////\\n\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 标准化\n",
    "# 将所有数据列转换为数值类型\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 使用 StandardScaler 进行标准化\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# 将标准化后的数据转换为 DataFrame\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分布分析\n",
    "# 计算每一列数据的偏度\n",
    "skewness = data_scaled.apply(lambda x: skew(x.dropna()))\n",
    "print(\"Skewness of each column after normalization:\")\n",
    "print(skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对每一列数据进行 Shapiro-Wilk 检验\n",
    "shapiro_results = data_scaled.apply(lambda x: shapiro(x.dropna())[1])\n",
    "print(\"Shapiro-Wilk test p-values for each column:\")\n",
    "print(shapiro_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关性分析\n",
    "# 计算每一列变量间的相关系数矩阵（皮尔逊相关系数）\n",
    "correlation_matrix = data_scaled.corr(method='pearson')\n",
    "print(\"Pearson correlation matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# 找出皮尔逊相关系数矩阵中大于0.9的项对应的两列\n",
    "high_corr_pairs = [(correlation_matrix.columns[i], correlation_matrix.columns[j]) \n",
    "                   for i in range(len(correlation_matrix.columns)) \n",
    "                   for j in range(i+1, len(correlation_matrix.columns)) \n",
    "                   if abs(correlation_matrix.iloc[i, j]) > 0.9]\n",
    "\n",
    "print(\"Pairs of columns with Pearson correlation coefficient > 0.9:\")\n",
    "print(high_corr_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主成分分析\n",
    "pca = PCA(n_components=2)  # 选择前两个主成分\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "\n",
    "# 将主成分转换为 DataFrame\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "\n",
    "# 打印主成分分析结果的前几行\n",
    "print(principal_df.head())\n",
    "\n",
    "# 打印主成分的解释方差\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"Explained variance of each principal component:\")\n",
    "print(explained_variance)\n",
    "\n",
    "# 绘制主成分分析结果的散点图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(principal_df['Principal Component 1'], principal_df['Principal Component 2'])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 Hopkins 统计量\n",
    "def hopkins(X):\n",
    "    d = X.shape[1]\n",
    "    n = len(X)\n",
    "    m = int(0.1 * n)  # heuristic from the article [1]\n",
    "    nbrs = NearestNeighbors(n_neighbors=1).fit(X)\n",
    "    rand_X = uniform(X.min(axis=0), X.max(axis=0), (m, d))\n",
    "    ujd = []\n",
    "    wjd = []\n",
    "    for j in range(0, m):\n",
    "        u_dist, _ = nbrs.kneighbors(rand_X[j].reshape(1, -1), 2, return_distance=True)\n",
    "        ujd.append(u_dist[0][1])\n",
    "        random_index = np.random.randint(0, n)\n",
    "        w_dist, _ = nbrs.kneighbors(X.iloc[random_index].values.reshape(1, -1), 2, return_distance=True)\n",
    "        wjd.append(w_dist[0][1])\n",
    "    H = sum(ujd) / (sum(ujd) + sum(wjd))\n",
    "    return H\n",
    "\n",
    "hopkins_stat = hopkins(data_scaled)\n",
    "print(\"Hopkins statistic:\", hopkins_stat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
